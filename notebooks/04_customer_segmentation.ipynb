{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c90434c-fdb3-48c4-b6a6-cb68efeff1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUSTOMER SEGMENTATION AND ADVANCED ANALYTICS\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape: (10000, 45)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. IMPORT LIBRARIES AND LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Market Basket Analysis\n",
    "try:\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "    MBA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MBA_AVAILABLE = False\n",
    "    print(\"mlxtend not installed. Run: pip install mlxtend\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure output folders exist\n",
    "os.makedirs('../outputs/figures', exist_ok=True)\n",
    "os.makedirs('../outputs/reports', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Load cleaned data (notebook is inside /notebooks)\n",
    "df = pd.read_csv('../data/processed/cleaned_retail_sales.csv')\n",
    "\n",
    "df['Order_Date'] = pd.to_datetime(df['Order_Date'])\n",
    "df['Ship_Date'] = pd.to_datetime(df['Ship_Date'])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CUSTOMER SEGMENTATION AND ADVANCED ANALYTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f5ea68-1c3e-414b-89a4-ee465b72f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFM table created: (1986, 8)\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "analysis_date = df['Order_Date'].max() + timedelta(days=1)\n",
    "\n",
    "rfm = df.groupby('Customer_ID').agg({\n",
    "    'Order_Date': lambda x: (analysis_date - x.max()).days,\n",
    "    'Order_ID': 'count',\n",
    "    'Sales': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['Customer_ID', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "rfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=[5,4,3,2,1], duplicates='drop')\n",
    "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "rfm['M_Score'] = pd.qcut(rfm['Monetary'], 5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "\n",
    "rfm['RFM_Score_Numeric'] = (\n",
    "    rfm['R_Score'].astype(int) +\n",
    "    rfm['F_Score'].astype(int) +\n",
    "    rfm['M_Score'].astype(int)\n",
    ") / 3\n",
    "\n",
    "print(\"RFM table created:\", rfm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf062f2-6c07-4a9e-b37d-4573fa20f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer_Segment\n",
      "Others           576\n",
      "Loyal            494\n",
      "Champions        353\n",
      "At Risk          344\n",
      "New Customers    219\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def segment_customer(row):\n",
    "    r, f, m = int(row['R_Score']), int(row['F_Score']), int(row['M_Score'])\n",
    "    if r >= 4 and f >= 4 and m >= 4:\n",
    "        return 'Champions'\n",
    "    elif r >= 3 and f >= 3:\n",
    "        return 'Loyal'\n",
    "    elif r >= 4 and f <= 2:\n",
    "        return 'New Customers'\n",
    "    elif r <= 2 and f >= 3:\n",
    "        return 'At Risk'\n",
    "    else:\n",
    "        return 'Others'\n",
    "\n",
    "rfm['Customer_Segment'] = rfm.apply(segment_customer, axis=1)\n",
    "\n",
    "print(rfm['Customer_Segment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0900112-e1d7-4e97-9969-2fb488a4bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rfm_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "rfm.to_csv('../data/processed/rfm_analysis.csv', index=False)\n",
    "print(\"Saved rfm_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02e0df0-29fc-460e-a294-71942eed7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2, silhouette=0.381\n",
      "k=3, silhouette=0.372\n",
      "k=4, silhouette=0.325\n",
      "k=5, silhouette=0.335\n",
      "k=6, silhouette=0.312\n",
      "k=7, silhouette=0.294\n",
      "Optimal k: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X = rfm[['Recency', 'Frequency', 'Monetary']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "scores = []\n",
    "for k in range(2, 8):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    scores.append(silhouette_score(X_scaled, labels))\n",
    "    print(f\"k={k}, silhouette={scores[-1]:.3f}\")\n",
    "\n",
    "optimal_k = range(2, 8)[scores.index(max(scores))]\n",
    "print(\"Optimal k:\", optimal_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468433eb-2a66-49be-b046-8607006655c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster\n",
      "1    1062\n",
      "0     924\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "rfm['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(rfm['Cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f0b493-5926-458c-bb7a-737026d68eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA variance explained: [0.70721995 0.24515155]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X_scaled)\n",
    "\n",
    "rfm['PCA1'] = components[:, 0]\n",
    "rfm['PCA2'] = components[:, 1]\n",
    "\n",
    "print(\"PCA variance explained:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892678ee-9fd4-4213-bbab-bbd6156eb136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved customer_segments.csv\n"
     ]
    }
   ],
   "source": [
    "rfm.to_csv('../data/processed/customer_segments.csv', index=False)\n",
    "print(\"Saved customer_segments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d2cb54-6886-455f-8f6a-0c1213fde586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frequent itemsets found at 1% support. MBA skipped.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('../outputs/reports', exist_ok=True)\n",
    "\n",
    "if MBA_AVAILABLE:\n",
    "\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "    transactions = df.groupby('Order_ID')['Product_ID'].apply(list).tolist()\n",
    "    te = TransactionEncoder()\n",
    "    encoded = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(encoded, columns=te.columns_)\n",
    "\n",
    "    # Try with reasonable support first\n",
    "    frequent_itemsets = apriori(\n",
    "        df_encoded,\n",
    "        min_support=0.01,\n",
    "        use_colnames=True\n",
    "    )\n",
    "\n",
    "    if frequent_itemsets.empty:\n",
    "        print(\"No frequent itemsets found at 1% support. MBA skipped.\")\n",
    "    else:\n",
    "        rules = association_rules(\n",
    "            frequent_itemsets,\n",
    "            metric='lift',\n",
    "            min_threshold=1\n",
    "        )\n",
    "\n",
    "        rules.to_csv('../outputs/reports/market_basket_rules.csv', index=False)\n",
    "        print(f\"Market basket rules saved ({len(rules)} rules)\")\n",
    "\n",
    "else:\n",
    "    print(\"Market Basket Analysis skipped (mlxtend not installed)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c933de95-8c9f-4f0b-b239-2b9e2a27ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort_Index   0         1         2         3         4         5         6   \\\n",
      "Cohort                                                                          \n",
      "2022-01       1.0  0.268212  0.316225  0.291391  0.311258  0.326159  0.304636   \n",
      "2022-02       1.0  0.308861  0.288608  0.283544  0.354430  0.296203  0.303797   \n",
      "2022-03       1.0  0.316151  0.323024  0.309278  0.326460  0.285223  0.323024   \n",
      "2022-04       1.0  0.323529  0.240196  0.299020  0.313725  0.274510  0.299020   \n",
      "2022-05       1.0  0.340741  0.288889  0.325926  0.259259  0.340741  0.266667   \n",
      "\n",
      "Cohort_Index        7         8         9         10        11        12  \\\n",
      "Cohort                                                                     \n",
      "2022-01       0.312914  0.263245  0.319536  0.296358  0.279801  0.269868   \n",
      "2022-02       0.346835  0.316456  0.336709  0.308861  0.326582  0.220253   \n",
      "2022-03       0.250859  0.240550  0.312715  0.353952  0.213058  0.000000   \n",
      "2022-04       0.303922  0.323529  0.328431  0.225490  0.000000  0.000000   \n",
      "2022-05       0.318519  0.311111  0.281481  0.000000  0.000000  0.000000   \n",
      "\n",
      "Cohort_Index        13  \n",
      "Cohort                  \n",
      "2022-01       0.221854  \n",
      "2022-02       0.000000  \n",
      "2022-03       0.000000  \n",
      "2022-04       0.000000  \n",
      "2022-05       0.000000  \n"
     ]
    }
   ],
   "source": [
    "df_cohort = df.copy()\n",
    "df_cohort['Order_Month'] = df_cohort['Order_Date'].dt.to_period('M')\n",
    "df_cohort['Cohort'] = df_cohort.groupby('Customer_ID')['Order_Date'].transform('min').dt.to_period('M')\n",
    "df_cohort['Cohort_Index'] = (df_cohort['Order_Month'] - df_cohort['Cohort']).apply(lambda x: x.n)\n",
    "\n",
    "retention = (\n",
    "    df_cohort.groupby(['Cohort', 'Cohort_Index'])['Customer_ID']\n",
    "    .nunique()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "retention_rate = retention.div(retention.iloc[:, 0], axis=0)\n",
    "print(retention_rate.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "215b6dbd-9983-449a-8717-231a282ffe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLV file saved\n"
     ]
    }
   ],
   "source": [
    "customer_metrics = df.groupby('Customer_ID').agg({\n",
    "    'Sales': 'sum',\n",
    "    'Order_ID': 'count',\n",
    "    'Order_Date': ['min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "customer_metrics.columns = ['Customer_ID', 'Revenue', 'Orders', 'First', 'Last']\n",
    "\n",
    "customer_metrics['Lifespan_Years'] = (\n",
    "    (customer_metrics['Last'] - customer_metrics['First']).dt.days / 365\n",
    ").clip(lower=0.1)\n",
    "\n",
    "customer_metrics['CLV'] = (\n",
    "    customer_metrics['Revenue'] / customer_metrics['Orders']\n",
    ") * (customer_metrics['Orders'] / customer_metrics['Lifespan_Years']) * 3\n",
    "\n",
    "customer_metrics.to_csv('../data/processed/customer_clv.csv', index=False)\n",
    "print(\"CLV file saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83422224-0707-4bd0-850e-c516f414bbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
