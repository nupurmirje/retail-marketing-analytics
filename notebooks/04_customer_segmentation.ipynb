{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c90434c-fdb3-48c4-b6a6-cb68efeff1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUSTOMER SEGMENTATION AND ADVANCED ANALYTICS\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape: (10000, 45)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. IMPORT LIBRARIES AND LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Market Basket Analysis\n",
    "try:\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "    MBA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MBA_AVAILABLE = False\n",
    "    print(\"mlxtend not installed. Run: pip install mlxtend\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure output folders exist (PROJECT STANDARD)\n",
    "os.makedirs(\"../figures\", exist_ok=True)\n",
    "os.makedirs(\"../reports\", exist_ok=True)\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Load cleaned data (notebook is inside /notebooks)\n",
    "df = pd.read_csv(\"../data/processed/cleaned_retail_sales.csv\")\n",
    "\n",
    "# Convert date columns\n",
    "df[\"Order_Date\"] = pd.to_datetime(df[\"Order_Date\"])\n",
    "df[\"Ship_Date\"] = pd.to_datetime(df[\"Ship_Date\"])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CUSTOMER SEGMENTATION AND ADVANCED ANALYTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f5ea68-1c3e-414b-89a4-ee465b72f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CUSTOMER SEGMENTATION AND ADVANCED ANALYTICS\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape: (10000, 45)\n",
      "RFM table created successfully\n",
      "RFM Shape: (1986, 8)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. RFM FEATURE ENGINEERING (CUSTOMER-LEVEL)\n",
    "# ============================================================================\n",
    "\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CUSTOMER SEGMENTATION AND ADVANCED ANALYTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "\n",
    "# Ensure processed data directory exists\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Define analysis reference date\n",
    "analysis_date = df[\"Order_Date\"].max() + timedelta(days=1)\n",
    "\n",
    "# Create RFM table\n",
    "rfm = df.groupby(\"Customer_ID\").agg({\n",
    "    \"Order_Date\": lambda x: (analysis_date - x.max()).days,  # Recency\n",
    "    \"Order_ID\": \"count\",                                     # Frequency\n",
    "    \"Sales\": \"sum\"                                           # Monetary\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = [\"Customer_ID\", \"Recency\", \"Frequency\", \"Monetary\"]\n",
    "\n",
    "# RFM scoring\n",
    "rfm[\"R_Score\"] = pd.qcut(\n",
    "    rfm[\"Recency\"], 5, labels=[5, 4, 3, 2, 1], duplicates=\"drop\"\n",
    ")\n",
    "\n",
    "rfm[\"F_Score\"] = pd.qcut(\n",
    "    rfm[\"Frequency\"].rank(method=\"first\"),\n",
    "    5,\n",
    "    labels=[1, 2, 3, 4, 5],\n",
    "    duplicates=\"drop\"\n",
    ")\n",
    "\n",
    "rfm[\"M_Score\"] = pd.qcut(\n",
    "    rfm[\"Monetary\"], 5, labels=[1, 2, 3, 4, 5], duplicates=\"drop\"\n",
    ")\n",
    "\n",
    "# Composite RFM score\n",
    "rfm[\"RFM_Score_Numeric\"] = (\n",
    "    rfm[\"R_Score\"].astype(int)\n",
    "    + rfm[\"F_Score\"].astype(int)\n",
    "    + rfm[\"M_Score\"].astype(int)\n",
    ") / 3\n",
    "\n",
    "print(\"RFM table created successfully\")\n",
    "print(\"RFM Shape:\", rfm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf062f2-6c07-4a9e-b37d-4573fa20f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RFM-BASED CUSTOMER SEGMENTATION\n",
      "================================================================================\n",
      "\n",
      "Customer Segment Distribution:\n",
      "Customer_Segment\n",
      "Others           576\n",
      "Loyal            494\n",
      "Champions        353\n",
      "At Risk          344\n",
      "New Customers    219\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. CUSTOMER SEGMENT ASSIGNMENT (RFM-BASED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RFM-BASED CUSTOMER SEGMENTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def segment_customer(row):\n",
    "    r, f, m = int(row[\"R_Score\"]), int(row[\"F_Score\"]), int(row[\"M_Score\"])\n",
    "    if r >= 4 and f >= 4 and m >= 4:\n",
    "        return \"Champions\"\n",
    "    elif r >= 3 and f >= 3:\n",
    "        return \"Loyal\"\n",
    "    elif r >= 4 and f <= 2:\n",
    "        return \"New Customers\"\n",
    "    elif r <= 2 and f >= 3:\n",
    "        return \"At Risk\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "rfm[\"Customer_Segment\"] = rfm.apply(segment_customer, axis=1)\n",
    "\n",
    "print(\"\\nCustomer Segment Distribution:\")\n",
    "print(rfm[\"Customer_Segment\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0900112-e1d7-4e97-9969-2fb488a4bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rfm_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "rfm.to_csv('../data/processed/rfm_analysis.csv', index=False)\n",
    "print(\"Saved rfm_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02e0df0-29fc-460e-a294-71942eed7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "K-MEANS CLUSTERING: SILHOUETTE ANALYSIS\n",
      "================================================================================\n",
      "k = 2, silhouette score = 0.381\n",
      "k = 3, silhouette score = 0.372\n",
      "k = 4, silhouette score = 0.325\n",
      "k = 5, silhouette score = 0.335\n",
      "k = 6, silhouette score = 0.312\n",
      "k = 7, silhouette score = 0.294\n",
      "\n",
      "Optimal number of clusters selected: 2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. K-MEANS CLUSTERING – OPTIMAL CLUSTER SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K-MEANS CLUSTERING: SILHOUETTE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Select features for clustering\n",
    "X = rfm[[\"Recency\", \"Frequency\", \"Monetary\"]]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Silhouette analysis to find optimal k\n",
    "k_values = list(range(2, 8))\n",
    "scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels)\n",
    "    scores.append(score)\n",
    "    print(f\"k = {k}, silhouette score = {score:.3f}\")\n",
    "\n",
    "# Select optimal k\n",
    "optimal_k = k_values[scores.index(max(scores))]\n",
    "print(f\"\\nOptimal number of clusters selected: {optimal_k}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "468433eb-2a66-49be-b046-8607006655c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL CUSTOMER CLUSTER ASSIGNMENT\n",
      "================================================================================\n",
      "\n",
      "Cluster Distribution:\n",
      "Cluster\n",
      "1    1062\n",
      "0     924\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. FINAL K-MEANS CLUSTER ASSIGNMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL CUSTOMER CLUSTER ASSIGNMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Fit K-Means with optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "rfm[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(rfm[\"Cluster\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f0b493-5926-458c-bb7a-737026d68eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PCA FOR CLUSTER VISUALIZATION\n",
      "================================================================================\n",
      "PCA variance explained by components:\n",
      "[0.70721995 0.24515155]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. DIMENSIONALITY REDUCTION USING PCA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PCA FOR CLUSTER VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Store PCA components\n",
    "rfm[\"PCA1\"] = components[:, 0]\n",
    "rfm[\"PCA2\"] = components[:, 1]\n",
    "\n",
    "print(\"PCA variance explained by components:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892678ee-9fd4-4213-bbab-bbd6156eb136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved customer_segments.csv\n"
     ]
    }
   ],
   "source": [
    "rfm.to_csv('../data/processed/customer_segments.csv', index=False)\n",
    "print(\"Saved customer_segments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d2cb54-6886-455f-8f6a-0c1213fde586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MARKET BASKET ANALYSIS\n",
      "================================================================================\n",
      "No frequent itemsets found at 1% support. MBA skipped.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 7. MARKET BASKET ANALYSIS (ASSOCIATION RULE MINING)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../reports\", exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MARKET BASKET ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if MBA_AVAILABLE:\n",
    "\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "    # Prepare transaction data\n",
    "    transactions = df.groupby(\"Order_ID\")[\"Product_ID\"].apply(list).tolist()\n",
    "\n",
    "    te = TransactionEncoder()\n",
    "    encoded = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(encoded, columns=te.columns_)\n",
    "\n",
    "    # Frequent itemset mining\n",
    "    frequent_itemsets = apriori(\n",
    "        df_encoded,\n",
    "        min_support=0.01,\n",
    "        use_colnames=True\n",
    "    )\n",
    "\n",
    "    if frequent_itemsets.empty:\n",
    "        print(\"No frequent itemsets found at 1% support. MBA skipped.\")\n",
    "    else:\n",
    "        rules = association_rules(\n",
    "            frequent_itemsets,\n",
    "            metric=\"lift\",\n",
    "            min_threshold=1\n",
    "        )\n",
    "\n",
    "        rules_path = \"../reports/market_basket_rules.csv\"\n",
    "        rules.to_csv(rules_path, index=False)\n",
    "        print(f\"Market basket rules saved ({len(rules)} rules)\")\n",
    "        print(f\"Saved to: {rules_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Market Basket Analysis skipped (mlxtend not installed)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c933de95-8c9f-4f0b-b239-2b9e2a27ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COHORT ANALYSIS: CUSTOMER RETENTION\n",
      "================================================================================\n",
      "Cohort Retention Rate (sample):\n",
      "Cohort_Index   0         1         2         3         4         5         6   \\\n",
      "Cohort                                                                          \n",
      "2022-01       1.0  0.268212  0.316225  0.291391  0.311258  0.326159  0.304636   \n",
      "2022-02       1.0  0.308861  0.288608  0.283544  0.354430  0.296203  0.303797   \n",
      "2022-03       1.0  0.316151  0.323024  0.309278  0.326460  0.285223  0.323024   \n",
      "2022-04       1.0  0.323529  0.240196  0.299020  0.313725  0.274510  0.299020   \n",
      "2022-05       1.0  0.340741  0.288889  0.325926  0.259259  0.340741  0.266667   \n",
      "\n",
      "Cohort_Index        7         8         9         10        11        12  \\\n",
      "Cohort                                                                     \n",
      "2022-01       0.312914  0.263245  0.319536  0.296358  0.279801  0.269868   \n",
      "2022-02       0.346835  0.316456  0.336709  0.308861  0.326582  0.220253   \n",
      "2022-03       0.250859  0.240550  0.312715  0.353952  0.213058  0.000000   \n",
      "2022-04       0.303922  0.323529  0.328431  0.225490  0.000000  0.000000   \n",
      "2022-05       0.318519  0.311111  0.281481  0.000000  0.000000  0.000000   \n",
      "\n",
      "Cohort_Index        13  \n",
      "Cohort                  \n",
      "2022-01       0.221854  \n",
      "2022-02       0.000000  \n",
      "2022-03       0.000000  \n",
      "2022-04       0.000000  \n",
      "2022-05       0.000000  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8. COHORT ANALYSIS – CUSTOMER RETENTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COHORT ANALYSIS: CUSTOMER RETENTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare cohort data\n",
    "df_cohort = df.copy()\n",
    "df_cohort[\"Order_Month\"] = df_cohort[\"Order_Date\"].dt.to_period(\"M\")\n",
    "df_cohort[\"Cohort\"] = (\n",
    "    df_cohort\n",
    "    .groupby(\"Customer_ID\")[\"Order_Date\"]\n",
    "    .transform(\"min\")\n",
    "    .dt.to_period(\"M\")\n",
    ")\n",
    "\n",
    "df_cohort[\"Cohort_Index\"] = (\n",
    "    df_cohort[\"Order_Month\"] - df_cohort[\"Cohort\"]\n",
    ").apply(lambda x: x.n)\n",
    "\n",
    "# Build retention matrix\n",
    "retention = (\n",
    "    df_cohort\n",
    "    .groupby([\"Cohort\", \"Cohort_Index\"])[\"Customer_ID\"]\n",
    "    .nunique()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Retention rate calculation\n",
    "retention_rate = retention.div(retention.iloc[:, 0], axis=0)\n",
    "\n",
    "print(\"Cohort Retention Rate (sample):\")\n",
    "print(retention_rate.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d609f88-1bba-441a-84f1-31ea53459776",
   "metadata": {},
   "source": [
    "## Customer Lifetime Value (CLV) Estimation\n",
    "\n",
    "This section estimates **Customer Lifetime Value (CLV)** using historical transaction data.\n",
    "CLV helps identify high-value customers and supports strategic decisions such as\n",
    "customer retention, personalized marketing, and resource allocation.\n",
    "\n",
    "**Approach used:**\n",
    "- Aggregate customer-level revenue and purchase frequency\n",
    "- Estimate customer lifespan based on first and last purchase dates\n",
    "- Compute CLV using a simplified historical-value formula\n",
    "\n",
    "**Why this matters:**\n",
    "- Enables prioritization of high-value customers\n",
    "- Complements RFM segmentation and clustering results\n",
    "- Provides a quantitative foundation for predictive modeling and business strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "215b6dbd-9983-449a-8717-231a282ffe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CUSTOMER LIFETIME VALUE (CLV) ANALYSIS\n",
      "================================================================================\n",
      "CLV file saved successfully: ../data/processed/customer_clv.csv\n",
      "CLV dataset shape: (1986, 7)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. CUSTOMER LIFETIME VALUE (CLV) ESTIMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CUSTOMER LIFETIME VALUE (CLV) ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Aggregate customer-level metrics\n",
    "customer_metrics = df.groupby(\"Customer_ID\").agg({\n",
    "    \"Sales\": \"sum\",\n",
    "    \"Order_ID\": \"count\",\n",
    "    \"Order_Date\": [\"min\", \"max\"]\n",
    "}).reset_index()\n",
    "\n",
    "customer_metrics.columns = [\n",
    "    \"Customer_ID\", \"Revenue\", \"Orders\", \"First\", \"Last\"\n",
    "]\n",
    "\n",
    "# Customer lifespan (in years)\n",
    "customer_metrics[\"Lifespan_Years\"] = (\n",
    "    (customer_metrics[\"Last\"] - customer_metrics[\"First\"]).dt.days / 365\n",
    ").clip(lower=0.1)\n",
    "\n",
    "# Simple CLV estimation\n",
    "customer_metrics[\"CLV\"] = (\n",
    "    (customer_metrics[\"Revenue\"] / customer_metrics[\"Orders\"])\n",
    "    * (customer_metrics[\"Orders\"] / customer_metrics[\"Lifespan_Years\"])\n",
    "    * 3\n",
    ")\n",
    "\n",
    "# Save CLV data\n",
    "clv_path = \"../data/processed/customer_clv.csv\"\n",
    "customer_metrics.to_csv(clv_path, index=False)\n",
    "\n",
    "print(f\"CLV file saved successfully: {clv_path}\")\n",
    "print(\"CLV dataset shape:\", customer_metrics.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83422224-0707-4bd0-850e-c516f414bbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
